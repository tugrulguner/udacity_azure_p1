# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparameters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary

This dataset contains bankmarketing data, where target is whether the client subscribed a term deposit or not, 'yes' or 'no'. Using the features provided in the dataset, aim of this project to find the best parameters using HyperDriveConfig and AutoML.

Best model is found as Voting by AutoML mode with >95% accuracy. 

## Scikit-learn Pipeline
Data is imported using by using TabularDataFactory from the given dataset url, and then processed as preprocessing through the clean_data function in train.py script. Then, it split it into train and test and then model creation followed by score calculation. All of these steps are coded in train.py script, which is used by by ScriptRunConfig together with args necessary for the script. This configuration is fed into HyperDriveConfig together with RandomParameterSampling on args, Bandit policy, primary metric and primary metric goal. Then this system submitted to the cluster.

**What are the benefits of the parameter sampler you chose?**
I used RandomSampling because it is fast and results in almost equal accuracies compared to Grid one.

**What are the benefits of the early stopping policy you chose?**
I used Bandit policy since it checks the relative score improvements, which allows algorithms to be able to keep continue as long as there is individual score improvements

## AutoML
AutoML just took a couple of parameters like cross-validation split number, task like classification, primary_metrics and data. Models are automatically generated by the algorithm itself, which includes ensemble methods from LighGBM to RandomForest, and scores are evaluated based on the picked model by the system

## Pipeline comparison
HyperDrive is a hyperparameter tuning system over a model that is already specified, like LinearRegression in this project. It is fast, and works solely on optimizing the given model. On the other hand, AutoML is almost fully automated model deciding system that performs model decision and parameter tuning. It is slow but better than HyperDrive in terms accuracy since AutoML uses model as a hyperparameter too.

## Future work
For HyperDrive, different models can be run for hyper parameter tuning, or one can first run AutoML, decide on the model, and can run HyperDrive for tiny adjustments in the parameters.
